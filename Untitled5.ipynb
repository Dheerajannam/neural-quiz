{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NReS_jEfzk_c",
        "outputId": "c1c62af3-7d7a-402a-dcb3-d05280f97b86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "469/469 [==============================] - 229s 486ms/step - loss: 0.1134 - val_loss: 0.0798\n",
            "Epoch 2/50\n",
            "469/469 [==============================] - 222s 473ms/step - loss: 0.0781 - val_loss: 0.0760\n",
            "Epoch 3/50\n",
            "469/469 [==============================] - 222s 472ms/step - loss: 0.0754 - val_loss: 0.0740\n",
            "Epoch 4/50\n",
            "469/469 [==============================] - 221s 470ms/step - loss: 0.0741 - val_loss: 0.0735\n",
            "Epoch 5/50\n",
            "469/469 [==============================] - 221s 472ms/step - loss: 0.0732 - val_loss: 0.0723\n",
            "Epoch 6/50\n",
            "469/469 [==============================] - 223s 475ms/step - loss: 0.0726 - val_loss: 0.0718\n",
            "Epoch 7/50\n",
            "368/469 [======================>.......] - ETA: 45s - loss: 0.0721"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the dataset (e.g., MNIST)\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Reshape the data to match the expected input shape of the model\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "# Add random noise to the training data\n",
        "noise_factor = 0.2\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "# Clip the values to ensure they are in the range [0, 1]\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=input_shape)\n",
        "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "pool1 = MaxPooling2D((2, 2), padding='same')(conv1)\n",
        "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "pool2 = MaxPooling2D((2, 2), padding='same')(conv2)\n",
        "encoded = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "\n",
        "# Decoder\n",
        "conv3 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "up1 = UpSampling2D((2, 2))(conv3)\n",
        "conv4 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1)\n",
        "up2 = UpSampling2D((2, 2))(conv4)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2)\n",
        "\n",
        "# Define the autoencoder model\n",
        "autoencoder = Model(inputs, decoded)\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "autoencoder.fit(x_train_noisy, x_train,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test_noisy, x_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "decoded_imgs = autoencoder.predict(x_test_noisy)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "mse = K.mean(K.square(x_test - decoded_imgs))\n",
        "print('Mean Squared Error:', mse)\n",
        "\n",
        "# Visualize the reconstructed images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Define the autoencoder model\n",
        "def create_autoencoder(learning_rate=0.001, loss='binary_crossentropy'):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    pool1 = MaxPooling2D((2, 2), padding='same')(conv1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    pool2 = MaxPooling2D((2, 2), padding='same')(conv2)\n",
        "    encoded = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "\n",
        "    conv3 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    up1 = UpSampling2D((2, 2))(conv3)\n",
        "    conv4 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1)\n",
        "    up2 = UpSampling2D((2, 2))(conv4)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2)\n",
        "\n",
        "    autoencoder = Model(inputs, decoded)\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "    return autoencoder\n",
        "\n",
        "# Create a KerasClassifier\n",
        "autoencoder_clf = KerasClassifier(build_fn=create_autoencoder, verbose=0)\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "losses = ['binary_crossentropy', 'mean_squared_error']\n",
        "param_grid = dict(learning_rate=learning_rates, loss=losses)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=autoencoder_clf, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_search = grid_search.fit(x_train_noisy, x_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_.model\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "decoded_imgs = best_model.predict(x_test_noisy)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "mse = K.mean(K.square(x_test - decoded_imgs))\n",
        "print('Mean Squared Error:', mse)\n",
        "\n",
        "# Visualize the reconstructed images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ]
    }
  ]
}
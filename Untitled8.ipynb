{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z3snRar-2Jo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist, imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Embedding, LSTM\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Ensure TensorFlow and other libraries are installed (replace 'your_environment' with your package manager)\n",
        "# Use 'pip install tensorflow keras scikit-learn' in your terminal/command prompt\n",
        "\n",
        "# Image Classification\n",
        "\n",
        "def create_image_model(learning_rate, batch_size, optimizer):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer(learning_rate), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Text Classification\n",
        "\n",
        "def create_text_model(learning_rate, batch_size, optimizer):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(5000, 128, input_length=200))  # Adjust embedding size and maxlen as needed\n",
        "    model.add(LSTM(64, return_sequences=False))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer(learning_rate), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Load Data (replace with your data paths if necessary)\n",
        "\n",
        "def load_image_data():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "def load_text_data():\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000, maxlen=200)\n",
        "    x_train = pad_sequences(x_train, maxlen=200)\n",
        "    x_test = pad_sequences(x_test, maxlen=200)\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "# Hyperparameter Grid Search\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'optimizer': [Adam, SGD]\n",
        "}\n",
        "\n",
        "\n",
        "def run_grid_search(create_model_fn, x_train, y_train, x_test, y_test):\n",
        "    grid_search = GridSearchCV(estimator=create_model_fn, param_grid=param_grid, cv=3)\n",
        "\n",
        "    # Define callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001)\n",
        "\n",
        "    # Fit the grid search with callbacks\n",
        "    grid_search.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), callbacks=[early_\n"
      ]
    }
  ]
}